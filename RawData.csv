,summary
0,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<p><strong>We are working on building the next $100M revenue Fintech company (founded in Europe)<br/>
<br/>
</strong></p><br/>
<p> <br/>
<br/>
<br/>
</p> <p>To do this, we know we need the <strong>best product in the world</strong> and that means we are obsessed by bringing together the most talented team possible; with diverse experiences, backgrounds and skills.<br/>
<br/>
</p> <p>We have a variety of backgrounds in our team, from people who joined us with zero experience (through the Code Academy - read more here) to those who have spent lots of time in huge companies with bags of experience.<br/>
<br/>
</p> <p>We are focused on delivering world class software products that deliver true value. With 1000's of enterprise customers like Expedia, Chubb, XPO - we must be doing something right. But the journey is just beginning... <strong>we have 100k users on our platform every month and handle more than $2 trillion of invoice transactions.<br/>
<br/>
</strong></p> <p>At approx. $35M revenue today and with about 250 Sidetraders worldwide, we are not an early-stage start-up, but with ambitious plans for growth, we are at an exciting time in our story; and need the right talent to help us achieve our goals.<br/>
<br/>
</p> <p><strong>Challenges you will face<br/>
<br/>
</strong></p><br/>
<ul> <li>Design and develop robust data pipelines. Data is at the heart of everything we do. The pipelines you build will power our analytics, machine learning and product features</li> <li>Build tools and automation capabilities for data pipelines. The data we ingest and the appetite to consume it is ever increasing - and therefore so is the importance of making data integration more reliable and scalable</li> <li>Extend our Data Warehouse and Data Lake to empower data-driven decisions</li> <li>Help build a data-platform to democratize data - we want to create a single source of truth of all our data, which is open to and consumable by everyone at Sidetrade</li> <li>Support implementation of secure design principles according to policies and standards of Information Security</li> </ul><br/>
<p><strong>How our teams are structured<br/>
<br/>
</strong></p><br/>
<p>We have three Tech Hubs at Sidetrade, one in Birmingham (UK), one in Paris (France) and one in Calgary (Canada). We work in small, autonomous teams mixed with highly skilled engineers, usually up to a maximum of 7. Each team (or Squad) has a Product Manager, Technical Lead, a mix of engineers and dedicated QA.<br/>
<br/>
</p><br/>
<p>Each Squad has the autonomy to decide how they work best, within a basic Agile framework and work on strategic, customer-focused projects that deliver huge value.<br/>
<br/>
</p><br/>
<p><strong>Guilds<br/>
<br/>
</strong></p><br/>
<p>As well as our day-to-day teams who focus on product delivery, we also have what we call Guilds. These Guilds or groups of like-minded engineers, focus on specific technologies to focus/harness passion on deep technical expertise as well as following latest market trends. Anybody can join a guild (UI/UX, API's &amp; Gateway, Data Engineering, Data Science, SRE, Test) - so either it's your passion or you just want to learn something new.<br/>
<br/>
</p><br/>
<p><strong>You should apply if:<br/>
<br/>
</strong></p> <ul> <li>You have at least 3 years' experience in Data Engineering</li> <li>You have excellent knowledge of SQL and Python</li> <li>You have a good understanding of relational and NoSQL databases (including data modelling, data warehousing)</li> <li>You are experienced in developing and supporting robust, automated and reliable data pipelines (maybe using technologies such as Kafka, Airflow, Celery etc.)</li> <li>You are familiar with Agile and DevOps frameworks</li> <li>You have sound knowledge of data architecture, scalability and security/compliance</li> </ul> <p><strong>What we can offer you:<br/>
<br/>
</strong></p><br/>
<ul> <li>The chance to deliver features to huge B2B customers that have 1000's of active users every day like Expedia, Nespresso, XPO, Manpower and many more.</li> <li>The chance to work for a European hyper-growth European AI company with recent acquisition in US.</li> <li>The chance to work in a hybrid-remote environment with the option to work from home and as well as our Boulogne Tech Hub, with regular travel opportunities (if you'd like).</li> <li>The chance to work in a Guild of your choice with dedicated time available to work together on Guild-based tasks, like research, learning, mini hackathons etc.</li> <li>Full access to content on Udemy so that you can self-learn,</li> <li>The opportunity to work remotely from France and onsite</li> <li>The chance to work in an international environment speaking english with your Birmingham collegues on a daily basis</li> <li>25 days holiday + bank holidays + RTT</li> <li>Healthcare plan</li> </ul> <p>If you have previous experience as a Data Engineer and are looking for the next step in your career, we would love to hear from you!<br/>
<br/>
</p>
<!-- --> </span>
</div>"
1,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<strong><u>About Morgan Stanley<br/>
<br/>
</u></strong>Morgan Stanley is a leading global financial services firm providing a wide range of investment banking, securities, investment management and wealth management services. The Firm's employees serve clients worldwide including corporations, governments, and individuals from more than 1,200 offices in 43 countries.<br/>
<br/>
As a market leader, the talent and passion of our people is critical to our success. Together, we share a common set of values rooted in integrity, excellence, and strong team ethic. Morgan Stanley can provide a superior foundation for building a professional career – a place for people to learn, to achieve and grow. A philosophy that balances personal lifestyles, perspectives and needs is an important part of our culture.<br/>
<br/>
<strong>Risk and Data Analytics Centre, Paris<br/>
<br/>
</strong>As we see increasing client interest in quantitative strategies, maintaining, and enhancing our leading position in risk analytics and technology is critical to our Institutional Securities Group (ISG) businesses globally. The seamless collaboration between technologists and ISG personnel, including data scientists, financial engineers and quantitative analysts helps to deliver innovative tools and solutions that differentiate us from our competitors.<br/>
<br/>
Morgan Stanley is establishing a new Risk &amp; Data Analytics (“R&amp;D”) Centre in Paris, where it will support our growth strategy for ISG both in EMEA and more broadly. The Centre will deliver cutting-edge, proprietary solutions to address the rapidly changing business needs of our sales and trading franchise while providing a platform to foster innovation for our markets businesses, driven by analytical and technological excellence.<br/>
<br/>
<strong>Technology at Morgan Stanley <br/>
<br/>
</strong>Technology is the key differentiator that ensures that we manage our global businesses and serve clients on a market-leading platform that is resilient, safe, efficient, smart, fast and flexible. Technology redefines how we do business in global, complex and dynamic financial markets. We have a large number of award winning technology platforms that help to propel our Firm’s businesses to be the top in the market. We have built strong techno-functional teams which partner with our offices globally taking global ownership of systems and products. We have a vibrant and diverse mix of technologists working on different technologies and functional domains. There is a large focus on innovation, inclusion, giving back to the community and sharing knowledge.<br/>
<br/>
<strong>What will you be doing?<br/>
<br/>
</strong>A Data Analytics and Platform team within the Institutional Securities Technology division is seeking a collaborative, hands-on developer to build best-in-class solutions for Data APIs, Data Governance, Data Quality, Security &amp; Privacy, and Architecture. This role offers a unique opportunity to work in a state-of-the-art modern data stack using open-source technologies aligned with our cloud strategy. We are looking for a proficient developer who is excited about innovation, with a proven track record of delivery.<br/>
<br/>
<strong><u>Roles And Responsibilities Include<br/>
</u></strong><ul><li>You will work closely with quants and traders on multiple trading desks to design, develop, deploy and support the innovative data science environment </li><li>Design widely used and flexible APIs to our core data and functionality </li><li>Analyze and optimize performance of large data workloads using compute clusters </li><li>Stay up to date with emerging trends and tools in the data &amp; analytics domain </li><li>Provide support and design advice to users of the cross-asset platform </li><li>Work closely with strategists and other stakeholders to help move their financial assets, valuation models and data pipelines to the platform </li><li>Efficient Communication across regions and functions <br/>
</li></ul><strong><u>What We’re Looking For<br/>
<br/>
</u></strong>We have a number of roles available for a range of technologists at different experience levels to join the team who will need;<br/>
<ul><li>Excellent problem solving and code development skills </li><li>Ability and interest to research, learn and implement new Data and Analytics technologies and paradigms </li><li>Enterprise level software development practices </li><li>Strong oral and written communication skills </li><li>Strong team working ability in local and global teams </li><li>Passion for continuous improvement both personally and as a team <br/>
</li></ul><strong><u>Skills That Will Help You In The Role<br/>
</u></strong><ul><li>Experience with at least one of the following technologies: Python and/or Java </li><li>Experience with performance optimization and concurrent programming </li><li>Exposure to Flask/Numpy/pandas/Plotly/Jupyter/Airflow and general data science stack experience </li><li>Exposure to Natural Language Processing, Machine Learning and other advanced methods of Data Management </li><li>Exposure to grid distributed applications </li><li>Exposure to KDB or other timeseries database technologies </li><li>Exposure to Cloud technologies </li><li>Exposure to Full Stack Web Development </li><li>Exposure to Risk Management systems and a wide range of Financial Instruments <br/>
</li></ul><strong>Where will you be working?<br/>
<br/>
</strong>61, rue de Monceau, 75008 Paris<br/>
<br/>
We integrate global expertise with regional knowledge of France to offer clients outstanding services in investment banking, sales and trading, real estate and distribution of investment management products<br/>
<br/>
<strong><u>Flexible Work Statement<br/>
<br/>
</u></strong>Interested in flexible working opportunities? Morgan Stanley empowers employees to have greater freedom of choice through flexible working arrangements. Speak to our recruitment team to find out more.<br/>
<br/>
<strong><u>Equal Opportunities Statement<br/>
<br/>
</u></strong>Morgan Stanley is an equal opportunities employer. We work to provide a supportive and inclusive environment where all individuals can maximize their full potential. Our skilled and creative workforce is comprised of individuals drawn from a broad cross section of the global communities in which we operate and who reflect a variety of backgrounds, talents, perspectives, and experiences. Our strong commitment to a culture of inclusion is evident through our constant focus on recruiting, developing, and advancing individuals based on their skills and talents.<br/>
<br/>
<strong>Posting Date<br/>
<br/>
</strong>Aug 8, 2022<br/>
<br/>
<strong>Primary Location<br/>
<br/>
</strong>Europe, Middle East, Africa-France-France-Paris<br/>
<br/>
<strong>Education Level<br/>
<br/>
</strong>Bachelor's Degree<br/>
<br/>
<strong>Job<br/>
<br/>
</strong>Engineering<br/>
<br/>
<strong>Employment Type<br/>
<br/>
</strong>Full Time<br/>
<br/>
<strong>Job Level<br/>
<br/>
</strong>Vice President
<!-- --> </span>
</div>"
2,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<p><strong>For details about the offer, see </strong>https://greenly.welcomekit.co/jobs/climate-data-engineer_paris</p>
<!-- --> </span>
</div>"
3,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<p><strong>Who We Are</strong></p><p>Boston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.</p><p>BCG GAMMA combines innovative skills in computer science, artificial intelligence (AI), statistics, and machine learning with deep industry and functional expertise. The BCG GAMMA team is comprised of world-class data scientists, software and data engineers, as well as product experts with a consulting mindset.</p><p>We specialize in creating competitive advantage for our clients through AI driven transformations. Our teams own the full analytics value-chain end to end: framing new business challenges, designing innovative algorithms, implementing and deploying scalable solutions, and enabling colleagues and clients to fully embrace AI. Our product offerings span from fully custom-build to industry specific leading edge AI software solutions.</p><p><br/>
</p><p><strong>What You'll Do</strong></p><p>As a forward-deployed Data Engineer, you’ll be part of our rapidly growing engineering team and help build the next generation of AI solutions. You’ll have the chance to partner with clients in a variety of BCG regions and industries, and on key topics like climate change, enabling them to design, build, and deploy new and innovative solutions. Additional responsibilities will include developing and delivering thought leadership in scientific communities and papers as well as leading conferences on behalf of BCG GAMMA.</p><p><br/>
</p><p><strong>Who You Are</strong></p><p>We are looking for talented individuals with a passion for data engineering, software development and transforming organizations into AI led innovative companies</p><p>· Apply data engineering practices and standards to develop robust and maintainable solutions</p><p>· Actively involved in every part of the software development life cycle</p><p>· Experienced at guiding non-technical teams and consultants in best practices for large-scale data engineering</p><p>· Motivated by a fast-paced, service-oriented environment and interacting directly with clients on new features for future product releases</p><p>· Enjoy collaborating in teams to share software design and solution ideas</p><p>· A natural problem-solver and intellectually curious across a breadth of industries and topics</p><p>What You'll Bring (Experience &amp; Qualifications)</p><p>· Master’s Degree in Computer Science or relevant field</p><p>· Experience in data engineering and working with global and remote agile squads</p><p>· Proficiency with analytic software programming ideally in python, C++, or SCALA</p><p>· Fluency with the storage, manipulation, and management of relational, non-relational and streaming data structures, specifically SQL, Spark, and Hadoop</p><p>· Proficiency with infrastructure as code principles</p><p>· Experience working on AWS, Azure, or Google cloud infrastructure</p><p><br/>
</p><p><strong>NICE TO HAVE</strong></p><p>· DevOps: Docker, Kubernetes, CI/CD, Terraform</p><p>· Understanding of parallel computing</p><p>· Full stack development: GraphQL, React, JavaScript, TypeScript</p><p>· Data Science and machine learning (Pandas, Scikit learn)</p><p><br/>
</p><p><strong>WORK ENVIRONMENT</strong></p><p>· Fluency in English is required as well as fluency in the local language for most locations</p><p>· Position is located in either Paris, London, Amsterdam, Germany, Nordics, Warsaw, Moscow, Milan, Zurich or Madrid</p><p>· Ability to travel based on client and business needs. Expect 30-50%</p>
<!-- --> </span>
</div>"
4,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
                Position<p><br/>
</p>Votre rôle en tant que Stagiaire Data Engineer sera de participer à la construction du nouvel environnement data de 24s.com : mise en place d’ETL (extraction et transformation des données), data lake, data warehouse, alerting, reportings.<p><br/>
</p><strong><u>Missions</u></strong><p><br/>
</p><ul><li>Participer aux développements des flux ETL de la plateforme Data :</li><li>Collecter la donnée (source interne, externe) et la stocker sur le data lake Cloud Storage</li><li>Transformer et enrichir la donnée dans notre data warehouse BigQuery</li><li>Mettre en place le monitoring adapté et assurer l’intégrité du flux</li><li>Participer à la documentation de la stack Data</li><li>Mettre en place des contrôles / alertes sur la cohérence et la qualité des données</li><li>Réaliser les dashboards sur la base des besoins définis par les équipes métiers</li></ul><p><br/>
</p>Vous pouvez être amené à intervenir sur d’autres missions :<p><br/>
</p><ul><li>Faire évoluer le tracking web/app nécessaire aux analyses / dashboards et recetter son bon fonctionnement</li><li>Réaliser des analyses de façon ad-hoc</li></ul><p><br/>
</p>Pourquoi postuler ? :<p><br/>
</p>Rejoindre 24S en tant que Stagiaire Data c’est saisir l’opportunité de travailler sur des technologies récentes (Google Cloud Platform, AWS) dans une entreprise en pleine croissance.<p><br/>
</p>Profile<p><br/>
</p><ul><li>Vous êtes en dernière année d’école d’ingénieur ou équivalent universitaire</li><li>Vous avez d’excellentes compétences en Python et SQL</li><li>Vous êtes curieux, dynamique et rigoureux</li><li>Idéalement, vous connaissez déjà l’environnement Google Cloud Platform et du développement javascript</li></ul><p><br/>
</p>La maison 24S reconnait et recrute tous les talents.<p><br/>
</p><ul><li>Contract type: Stage</li></ul>
<!-- --> </span>
</div>"
5,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<strong>Concrètement votre quotidien ? </strong><p><br/>
</p>Ce poste consistera à mettre à disposition de STFS les données nécessaires aux stress tests crédit.<p><br/>
</p>En tant que Data Engineer, vous comprendrez les besoins nécessaires pour l'exécution des différents exercices de stress tests. Vous participerez à la formulation de la demande/spécification du besoin, collecterez la donnée, la traiterez, l'analyserez et la structurerez.<p><br/>
</p>Cela demandera une connaissance fonctionnelle du crédit et des besoins spécifiques liés à l'activité de l'équipe de modélisation de STFS (STMM).<p><br/>
</p>Les base de données, résultats de ce travail, seront fournies à STMM et au moteur de stress test afin de pouvoir effectuer des projections d'actifs pondérés et de coût du risque, dépendantes de scénarios macroéconomiques.<p><br/>
</p>Ces travaux vous demanderont un bon niveau de communication et une très bonne capacité de travail en équipe, au sein de STFS d'une part et avec les métiers (marchés domestiques, CIB, PF, IRB) et fonctions du groupe (RISK et FINANCE) d'autre part.<p><br/>
</p>La donnée est cœur du besoin des stress tests crédit. Dans un environnement réglementaire toujours plus exigeant, notre capacité à la préparer, la restituer dans des temps courts, avec une qualité optimale, sont clés.<p><br/>
</p><strong>L'environnement de travail, c'est important ! </strong><p><br/>
</p>BNP PARIBAS Group Finance, RISK and ALM Treasury ont mis en place une équipe en JV en charge des stress test Groupe, financial planning et la synthèse financière appelée STFS pour Stress Testing and Financial Synthesis.<p><br/>
</p>Au sein de la plateforme STFS, Stress Testing and Data Analytics (STDA), est responsable de la mise à disposition des données nécessaires à l'exécution des exercices de stress tests internes et réglementaires sur l'ensemble des risques matériels du groupe (crédit, titrisation, risque de marché du banking book...). STDA est aussi responsable de certains reportings réglementaires dans lesquels STFS est impliqué.<p><br/>
</p>L'équipe est composée d'experts fonctionnels afin de comprendre et formaliser le besoin de données, mais aussi techniques afin de structurer, contrôler, développer des solutions de mise à disposition de ces données. Sur le crédit, les bases de données moteur sont mises à disposition du moteur de stress test groupe pour fournir des projections d'actifs pondérés et de coût du risque. STDA est un des principaux utilisateurs de la plateforme de stress testing du groupe (PROMETHEE).<p><br/>
</p>Le poste est basé au Millénaire 1, Paris 19ème. Vous pourrez télétravaillez jusqu'à 2,5 jours par semaine.<p><br/>
</p><strong>Et après ? </strong><p><br/>
</p>Cette position transverse offre de nombreuses opportunités de travailler avec des acteurs dans de nombreuses régions et d'appréhender et comprendre les différents métiers dans le Groupe.<p><br/>
</p><strong>Et la rémunération ?</strong><p><br/>
</p>Nous saurons valoriser votre talent. Discutons-en !<p><br/>
</p><strong>Pourquoi BNP Paribas ? </strong><p><br/>
</p>Notre monde change : notre manière de nous informer, de consommer… et de travailler aussi ! Aujourd’hui, ce qui compte dans un job, c’est de vivre de véritables expériences, d’apprendre, de partager objectifs et résultats avec ses collègues.<p><br/>
</p>Bref, de tracer son propre chemin, différent, responsable et durable.<p><br/>
</p>Chez BNP Paribas, nous recrutons nos collaborateurs avec l’idée qu’ils nous aideront à concevoir le monde et la banque de demain.<p><br/>
</p>Vous voulez connaître toutes les raisons de nous rejoindre ? Rendez-vous sur www.bnpparibas.com<p><br/>
</p><strong>Et vous ? </strong><p><br/>
</p>Vous êtes diplômé d’un Bac+5 et vous avez une expérience de 5 ans minimum en tant que Data analyst en risque/finance. Vous maîtrisez SQL et Python.<p><br/>
</p>Votre niveau d’anglais est avancé aussi bien à l’oral qu’à l’écrit.<p><br/>
</p>Vous avez développé de bonnes capacités à collaborer et à communiquer.<p><br/>
</p>Votre rigueur, votre capacité d’analyse alliées à votre capacité d’organisation seront des atouts pour réussir sur ce poste.<p><br/>
</p>Dans un monde qui change, la diversité, l’équité et l’inclusion sont des valeurs clés pour le bien-être et la performance des équipes.<p><br/>
</p>Chez BNP Paribas, nous souhaitons accueillir et retenir tous les talents sans distinction : c’est ainsi que nous construirons, ensemble, la finance de demain, innovante, responsable et durable.<p><br/>
</p>Enfin, nous attachons une importance particulière à ce que nos futurs collaborateurs agissent au quotidien avec responsabilité éthique et professionnelle.<p><br/>
</p>À tout moment pendant le processus de recrutement, les informations figurant sur votre CV, vos données d'identification et vos antécédents pourront être vérifiés.<p><br/>
</p><strong>Lieu principal</strong><p><br/>
</p>FR-Île-de-France-PARIS<p><br/>
</p><strong>Type d'emploi</strong><p><br/>
</p>CDI<p><br/>
</p><strong>Domaine d'activité</strong><p><br/>
</p>EFFICACITÉ OPÉRATIONNELLE TRANSFORMATION ET MOA (OU BA)<p><br/>
</p><strong>Niveau d%27Etudes</strong><p><br/>
</p>Master ou équivalent (&gt; 4 ans)<p><br/>
</p><strong>Niveau d%27expérience</strong><p><br/>
</p>Au moins 5 ans<p><br/>
</p><strong>Horaire</strong><p><br/>
</p>Temps plein<p><br/>
</p><strong>Langue (1)</strong><p><br/>
</p>Anglais
<!-- --> </span>
</div>"
6,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<p>Dailymotion is seekinga Data(Analytics)Engineer for the Analytics Engineering team.<br/>
<br/>
</p><p>You will join the Data Engineering &amp; Machine Learning craft. A craft consists of multiple teams of engineers and machine learning experts who collaborate daily to create and run Data products in Dailymotion. Inside this craft, the Analytics Engineering team’s mission is to provide trustworthy and available data to enable analysis &amp; insights throughout the company (B2C, B2B products, and business teams).<br/>
<br/>
</p><p>Analytics Engineering team builds and maintains products likeourmulti-petabyte data warehouse, event processors (at tens of thousands of messages per second), highly scalable client-facing analytics, data ingestion &amp; distribution, synchronizing data across databases &amp; systems, etc. The team is responsible for making costs-performance tradeoffs around data modeling &amp; architecture. The team is also involved with training users of our data on SQL and analytics best practices and spearheading a significant effort around data governance.<br/>
<br/>
</p><p>Analytics Engineering is a new and emerging space within the Data sphere. As an Analytics Engineer, you bring a software engineering mindset, best practices to maintain analytics code, and to model data from its source to its use in the data warehouse as business and reporting data. It requires a mix of programming skills and data skills on a day-to-day basis. If you are interested in solving challenging business problems with your skills, consider applying to this role. Your impact will be broad and across all of Dailymotion’s businesses.<br/>
<br/>
</p><p><strong>Whatyou will do:<br/>
<br/>
</strong></p><ul><li><p>Collect vast amounts of raw data from internal sources and external sources in batch and streaming modes.<br/>
<br/>
</p></li></ul><ul><li><p>Expose the data through APIs, flat files, data marts, etc., for internal and external users.<br/>
<br/>
</p></li><li><p>Design Druid datasets for external facing consumers for speed, consistency, cost, and efficiency.<br/>
<br/>
</p></li><li><p>Write complex and optimal SQL queries to transform data in our data lake into reliable business entities and then into reporting aggregates. Identify dependencies for these transformations. Schedule these transformations through Airflow.<br/>
<br/>
</p></li><li><p>Investigate data discrepancy, data quality issues. Debug performance issues using query plan.<br/>
<br/>
</p></li><li><p>DesignBigQuerytable data model to efficiently answer business use cases considering cost and performance.<br/>
<br/>
</p></li></ul><ul><li><p>Ensure data is clean, consistent, and available. Perform data quality checks, create monitors.<br/>
<br/>
</p></li><li><p>Catalogand documentthe business entities, data marts, dimensions, metrics, business rules, etc.<br/>
<br/>
</p></li><li><p>Be a knowledge guide on the various business entities, data marts. Train users of our data on SQL and analytics best practices.<br/>
<br/>
</p></li><li><p>Come up with new tools, processes, documents and explore new tech during the cool-down periods.<br/>
<br/>
<br/>
<br/>
</p></li></ul><strong>Qualifications<br/>
<br/>
</strong><ul><li>BS/MS in Computer Science, Engineering or related field</li><li><p>2+ years experiencearound Big Data, Data warehousing, writing complex SQL, and debugging complex SQL.<br/>
<br/>
</p></li><li><p>1+ years of experience developing and debugging software inPython.<br/>
<br/>
</p></li></ul><ul><li><p>Good business modeling skills: going from a stakeholder’s expressed requirements to an actual data model.<br/>
<br/>
</p></li><li><p>Ability to work with multiple stakeholders-Product, Engineers, Analysts,Product managers, DevOps, etc.<br/>
<br/>
</p></li><li><p>Comfortable working with Linux and the GCP stack<br/>
<br/>
</p></li><li><p>Experience withPubSub, Data flow, Data Processor, Airflow or Kafka, Spark, or other streaming technologies is a plus.<br/>
<br/>
</p></li><li><p>Experience in real-time analytics databases like Apache Druid is a plus.<br/>
<br/>
</p></li></ul><ul><li><p>Familiarity with NoSQL technologies such as Aerospike is a plus.<br/>
<br/>
</p></li><li><p>Writing and speaking proficiency in English<br/>
<br/>
</p></li></ul><p><strong>Technologies used by the team:<br/>
</strong>Google Cloud Platform (BigQuery, Cloud Storage, Beam/Dataflow, Compute Engine,etc), Python, GO, Airflow, SQL, Git, Java, JSON, Bash, Docker, Druid, Kubernetes, etc<br/>
<br/>
<br/>
<br/>
</p><strong>Additional Information<br/>
<br/>
</strong><p>At Dailymotion, we empower candidates to take action. If this job sounds like a great opportunity for you, be confident in your skills, we are always happy to meet you! If needed, we can accommodate our recruitment process for your special abilities.<br/>
<br/>
</p><p>Location:Remote in France / Sophia Antipolis / Paris<br/>
Type of contract: Permanent<br/>
Start Date: ASAP<br/>
<br/>
</p><p><u>For the France offices 🇫🇷<br/>
<br/>
</u></p><p>🏡Hybrid Work Framework (4 types of remote work:Full office /Flex office (1/2 days remote) / Flex remote (1/2 days at the office) / Full remote + ability to work 3 months abroad)<br/>
💰International Group Savings Plan offered through the Vivendi Group<br/>
🍼8weeks paid Paternity leave or Co-parental leave<br/>
🕶️Excellent Employee Culture (Company Events / Training / Parties / All hands …)<br/>
🚀Career development support (training / career check-in with HR / internal mobility / compensation cycle / 360 quarter feedback review …)<br/>
🏥Company-paid Health Insurance andPersonal Services Vouchers (CESU)<br/>
🚆Commuter benefit coverage - Public Transport and Bike refund<br/>
⛱️Paid Time off – RTT and Saving time plan (CET)<br/>
✅Meal Vouchers<br/>
🎡Workers representatives committee(sports membership/cinemas vouchers/gift vouchers/discount)<br/>
<br/>
</p><p><br/>
<u>Feel free to explore Dailymotionculture a little further, please check out:<br/>
<br/>
</u></p><ul><li>Dailymotion.com</li><li>New-York office - BuiltIn</li><li>Offices in France - Welcome to the Jungle</li><li>Our articles</li></ul>
<!-- --> </span>
</div>"
7,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<strong><u>About Us</u></strong><p><br/>
</p><strong>Description</strong><p><br/>
</p><i>Kaspr is a SaaS business, helping salespeople, recruiters and marketers in their lead generation efforts. Kaspr is the leader of the French SMB market and is making major strides towards becoming the European leader in the space.</i><p><br/>
</p><i>The concept is simple - retrieve contact information from identified LinkedIn profiles, via a Google Chrome extension.</i><p><br/>
</p><i>There’s never been a more exciting time to join the Kaspr team – having recently been acquired by Cognism, Kaspr is expanding globally and rapidly having launched into 10 new countries in the past three months alone. Our people, culture (and vibrant new offices!) ensure all our employees feel empowered to succeed in their roles, while having fun along the way. We’d love to hear from you!</i><p><br/>
</p><strong><u>Summary</u></strong><p><br/>
</p>We are now looking for an mid-level Data Engineer to join our fast-growing team. You will be responsible for building new systems that collect, manage, and convert raw data into usable information to analyze.<p><br/>
</p>The ideal candidate is an experienced with big data pipelines and a natural data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our team and will ensure optimal data delivery architecture is consistent throughout on going projects.<p><br/>
</p>If you are detail-oriented, with excellent organizational skills and experience in this field, we’d like to hear from you.<p><br/>
</p><strong><u>Key Responsibilities</u></strong><p><br/>
</p>Building new systems that collect, manage, and convert raw data into usable information.<p><br/>
</p>Support our team on data initiatives<p><br/>
</p>Participate in all phases of software development including concept, design, prototyping, and production release<p><br/>
</p>Work directly with non-technical associates to understand business requirements<p><br/>
</p><strong><u>Requirements</u></strong><p><br/>
</p><strong>Our ideal candidate will:</strong><p><br/>
</p><ul><li> Have a 2+ years of experience as a software engineer</li><li> Have a degree in Computer Science, Computer Engineering, Applied Statistics, Mathematics, or a related field (or you are in the process of achieving one).</li><li> Have some programming experience with JavaScript/Python and NoSQL and are familiar with the principles of software engineering and data analytics.</li><li> Be excited to try new technologies and develop your technical skills.</li><li> Have know-how of designing and interacting with databases (SQL/NoSql).</li><li> Have experience with AWS-based database systems.</li><li> Be familiar with GIT and release engineering strategies.</li><li> Be proficient in spoken and written English.</li></ul><p><br/>
</p><strong><u>Benefits</u></strong><p><br/>
</p><ul><li>Insurance “Mutuelle” : 50% is covered by the company</li><li>Transport tickets : 50% is covered by the company</li><li>Swile card : 10€ per lunch ticket 50% is covered by the company</li><li>Coffee/tea/beer free</li><li>Remote work 2 days/month</li><li>Breakfast every Mondays</li><li>Lunch free once per month</li><li>Days off : Each employee earns 2.5 days off per month ( pro rate if missing working days ) total of 30 days per year. Friday counts as 2 days</li></ul>
<!-- --> </span>
</div>"
8,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<p>Votre rôle en tant que Stagiaire Data Engineer sera de participer à la construction du nouvel environnement data de 24s.com : mise en place d’ETL (extraction et transformation des données), data lake, data warehouse, alerting, reportings.</p><p><br/>
</p><p><u>Missions :</u></p><ul><li>Participer aux développements des flux ETL de la plateforme Data :</li><li>Collecter la donnée (source interne, externe) et la stocker sur le data lake Cloud Storage</li><li>Transformer et enrichir la donnée dans notre data warehouse BigQuery</li><li>Mettre en place le monitoring adapté et assurer l’intégrité du flux</li><li>Participer à la documentation de la stack Data</li><li>Mettre en place des contrôles / alertes sur la cohérence et la qualité des données</li><li>Réaliser les dashboards sur la base des besoins définis par les équipes métiers</li></ul><p>Vous pouvez être amené à intervenir sur d’autres missions :</p><ul><li>Faire évoluer le tracking web/app nécessaire aux analyses / dashboards et recetter son bon fonctionnement</li><li>Réaliser des analyses de façon ad-hoc</li></ul><p><br/>
</p><p>Pourquoi postuler ? :</p><p>Rejoindre 24S en tant que Stagiaire Data c’est saisir l’opportunité de travailler sur des technologies récentes (Google Cloud Platform, AWS) dans une entreprise en pleine croissance. </p><p><br/>
</p><p><u>Profil recherché :</u></p><ul><li>Vous êtes en dernière année d’école d’ingénieur ou équivalent universitaire</li><li>Vous avez d’excellentes compétences en Python et SQL</li><li>Vous êtes curieux, dynamique et rigoureux</li><li>Idéalement, vous connaissez déjà l’environnement Google Cloud Platform et du développement javascript</li></ul><p><br/>
</p><p>La maison 24S reconnait et recrute tous les talents.</p>
<!-- --> </span>
</div>"
9,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<p>Equancy est un cabinet de conseil international, basé à Paris et Dubaï, spécialisé dans la transformation data des entreprises.</p><p><br/>
</p><p>Nous planifions, concevons et mettons en œuvre des solutions Big Data, Data Science et Intelligence Artificielle pour nos clients. Nos projets vont de la mise en œuvre d’infrastructures spécialisées dans le traitement de la donnée de nos clients, de lacs de données jusqu’au développement de systèmes opérationnels intégrant des algorithmes de <i>machine learning</i> ou de <i>deep learning</i>. Nous sommes experts dans l’industrialisation de ces plates-formes, en appliquant les principes du devops à nos infrastructures data.</p><p><br/>
</p><p>Nos clients sont de grands groupes français et internationaux (LVMH, Picard, Chanel VINCI, Volkswagen). Ils nous font confiance autant dans l’accompagnement au cadrage de leurs besoins que dans la réalisation des solutions data innovantes.</p><p><br/>
</p><p>En tant que <strong>Data Engineer</strong>, rattaché au pôle Data Technologies d'Equancy, vous serez amené(e) à intervenir sur des missions variées :</p><p><br/>
</p><ul><li>Accompagnement projet dans les différentes étapes de build de plateformes (datalake, dashboards, …);</li><li>Conception de pipeline d'ingestion et traitement de données (cycle de vie de la donnée);</li><li>Développement de briques ETL, transformation de flux batch et temps réel;</li><li>Création de modèle de données;</li><li>Préparation à la mise en production de cas d'usage et industrialisation (accompagnement des data scientists, contribution à l'amélioration d'efficacité sur les déploiements);</li><li>Formation, évangélisation sur des technologies innovantes en interne et auprès de clients;</li><li>Contribution à la documentation des travaux;</li><li>Participation aux dossiers d’avant-ventes, à la rédaction des réponses et aux soutenances orales.</li></ul><p><br/>
</p><p>Vous assurerez également une veille technologique, et proposerez des solutions standardisées, fiables, évolutives et réplicables.</p><p><br/>
</p><p><strong>Profil recherché</strong></p><ul><li>De formation BAC+5 ingénieur ou université</li><li>Minimum 2 ans d’expérience sur des sujets Big Data, avec des réalisations de projets Datalake et BI/reporting;</li><li>Maîtrise à minima d'un des environnements Cloud AWS, GCP et Azure;</li><li>De bonnes compétences sur les sujets ou technologies suivantes:</li><li>Framework Spark (pySpark, Scala),</li><li>Pandas et optionnellement scikit-learn ;</li><li>Utilisation de Airflow ;</li><li>ETL ;</li><li>Api REST ;</li><li>Bases de données SGBDR / NoSQL ;</li><li>Docker ;</li><li>Scripting Python et Shell ;</li><li>Outils d’intégration continue tels que Terraform, Ansible et Jenkins ;</li><li>Outils de développement : jupyter Notebook, Pycharm, Git ;</li><li>Une expérience dans le développement en contexte agile serait un plus, ainsi que dans la gouvernance de donnée (RGPD) et la génération de data catalogue.</li></ul><p><br/>
</p><p>A propos d'Equancy, Equancy c’est :</p><ul><li>90 consultants</li><li>30 ans d’âge moyen</li><li>18 années d’existence</li><li>3 bureaux : Paris, Dubaï &amp; Mumbai</li><li>3 practices : Stratégie, Data Science &amp; Technologie, Marketing Performance</li><li>5 expertises sectorielles : Auto &amp; mobilité, Retail &amp; e-commerce, Services financiers, Tourisme &amp; entertainment, Consumer goods</li></ul><p><br/>
</p><p><br/>
</p><p><strong>Conditions de travail/cadre de travail</strong> :</p><ul><li>Poste à pourvoir dès que possible, CDI, statut Cadre;</li><li>Rémunération attractive;</li><li>Cadre de travail :</li></ul><p>· Superbes locaux au cœur de Paris : Espace WeWork Jules Lefebvre, à coté de Saint Lazare, au sein d’un bâtiment historique, avec de grands espaces et vue panoramique sur tout Paris;</p><p>· Equilibre vie pro / vie perso;</p><p>· Une politique de télétravail de deux jours par semaine;</p><p>· Équipement pour travailler en remote + participation aux frais du télétravail (allocation mensuelle);</p><p>· Engagement environnemental;</p><p>· Des activités sportives proposées par WeWork x Equancy;</p><p>· Une conciergerie proposée par We Work.</p><p><br/>
</p><p>Vous avez en vie de rejoindre un cabinet indépendant où l’esprit entrepreneurial, l’excellence et la bienveillance nous guident ? Alors écrivez-nous !</p>
<!-- --> </span>
</div>"
10,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<p>Nous sommes à la recherche d’un Big Data Engineer Snowflake Sénior qui sera en charge des modélisations data et de l’intégration des données: acquisition, préparation, modélisation et stockage, exposition, . Vous devrez faire preuve d’un état d’esprit à la fois innovant, méthodique, orienté solution (et non problème!), et communiquant.<br/>
<br/>
</p><p><strong>Responsabilités<br/>
<br/>
</strong></p><ul><li>Analyse des besoins techniques métiers, participation à la définition des architectures solution SQL, développement et optimisation, code review, maintenir les pratiques Devops “You build IT, You run IT”, support à recette et mise en production, documentation,…</li><li>Modélisation de la Cloud Database Snowflake</li><li>Benchmark de solutions et conseil auprès de notre client sur les solutions technologiques à adopter, en lien avec leurs besoins</li><li>Partage de connaissances et formations interne<br/>
<br/>
</li></ul><strong>Qualifications<br/>
<br/>
</strong><ul><li>Issu d’une formation supérieure (école d’ingénieur, master…) avec une expérience dans le domaine du conseil (orienté satisfaction client et vision partenariale)</li><li>Vous disposez d’au moins 6 années d’expérience dans le domaine du SQL/ETL et ayant une expérience d’au moins 1 an sur Snowflake</li><li>Maîtrise du développement data (SQL, Python, …) et vous disposez de solides expériences dans la mise en place de pipeline de données</li><li>Maîtrise d’au moins une technique de modélisation: Star Schéma, DataVault, DataMesh,…</li><li>Expérience sur une plateforme Cloud (idéalement AWS)</li><li>Expérience sur des flux temps réel</li><li>La connaissance de concepts comme les suivants serait un +: DataOps, FinOps,..</li><li>Expérience de l’Agilité</li><li>Autonomie, organisation, sens du partage</li><li>Bonne communication</li><li>Orientation produit et solution<br/>
<br/>
</li></ul><strong>Additional Information<br/>
<br/>
</strong><p><u>Avantages</u>:<br/>
<br/>
</p><p>· Plan de formation pour accompagner votre carrière (formations éditeurs, certifications) grâce à nos partenariats nous accordant une position de partenaire privilégié, et management de proximité par des experts<br/>
<br/>
</p><p>· Top 5 du Palmarès Great Place to Work<br/>
<br/>
</p><p>· Siège parisien situé à Charles-De-Gaulle Etoile<br/>
<br/>
</p><p>· Tickets restaurants<br/>
<br/>
</p><p>· Mutuelle d’entreprise prise en charge à 100%<br/>
<br/>
</p><p>· Prime vacances<br/>
<br/>
</p><p>· Prime de participation<br/>
<br/>
</p><p>· 1% logement<br/>
<br/>
</p><p>· RTT<br/>
<br/>
</p>
<!-- --> </span>
</div>"
11,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<p><strong>Our team context:<br/>
<br/>
</strong>Dailymotion is seeking a Junior Data Engineer to join the Data tribe. Our tribe is responsible for all the Data products at Dailymotion; your work will have an impact throughout Dailymotion’s business and help make data-driven decisions on products and strategy.<br/>
<br/>
You will join a team of Data and Machine Learning engineers who have built Software that processeshundreds of terabytes of data, billions of real-time events, hundreds of tasks automated by Airflow and millions of API calls every day. Multiple machine learning projects including recommender systems, semantic annotations, spam detection, and fraud detection.<br/>
<br/>
<strong>What we will do together:<br/>
<br/>
</strong>Our stack runs almost exclusively on Google Cloud Platform. You will work in an environment made up of Data Lakes (BigQuery, etc.), data streaming platforms (Beam / Dataflow, Flink, etc.), orchestration and scheduling platforms (Airflow), container-oriented deployment, and management platforms (Docker, K8S, Jenkins), SQL. You will also participate in data modeling activities and design of data flows until their implementation and support in production.<br/>
<br/>
<br/>
<br/>
</p><strong>Qualifications<br/>
<br/>
</strong><p><strong>Why you are a perfect candidate for us:<br/>
<br/>
</strong></p><p>• You have 1+ - 3years accumulated experience as a Junior Data Engineer<br/>
• You are fluent in English and French<br/>
• You are a team player, continually suggesting improvements and effective collaboration.<br/>
• You like to implement new technologies and innovative solutions as well as the associated prototyping.<br/>
• You know how to write technical specifications.<br/>
• You have hands-on experience or interest in building/managingBig Data pipelines.<br/>
• You will be motivated in working onbuilding batch and streaming data streams to process a largenumber of events.<br/>
• You have hands-on experience with different languages e.g.: Python, Go lang, Java, monitoring of development standards to ensure delivery of reusable and good quality code.<br/>
• You have hands-on experience with different types of databases and SQL knowledge.<br/>
• You have experience establishing and maintaining integration tests.<br/>
• Experience on how to automate your deployments (Docker, Kubernetes, CI, Datadog...)<br/>
• You have an entry-level knowledge onhow to analyze, design, or improve the efficiency, scalability, and stability of data collection, storage, and retrieval processes.<br/>
<br/>
</p><p><strong>What we offer you:<br/>
<br/>
</strong>• Additional opportunities as we grow and learn together.<br/>
• Join our open, collaborative culture.<br/>
• Exciting, dynamic projects to work on.<br/>
• Flexibility to work remotely.<br/>
<br/>
<br/>
<br/>
</p><strong>Additional Information<br/>
<br/>
</strong><p>At Dailymotion, we empower candidates to take action. If this job sounds like a great opportunity for you, be confident in your skills, we are always happy to meet you! If needed, we can accommodate our recruitment process for your special abilities.<br/>
<br/>
</p><p>Location: Paris<br/>
Type of contract: Permanent<br/>
Start Date: ASAP<br/>
<br/>
</p><p><u>For the France offices 🇫🇷<br/>
<br/>
</u></p><p>🏡Hybrid Work Framework (4 types of remote work:Full office /Flex office (1/2 days remote) / Flex remote (1/2 days at the office) / Full remote + ability to work 3 months abroad)<br/>
💰International Group Savings Plan offered through the Vivendi Group<br/>
🍼8weeks paid Paternity leave or Co-parental leave<br/>
🕶️Excellent Employee Culture (Company Events / Training / Parties / All hands …)<br/>
🚀Career development support (training / career check-in with HR / internal mobility / compensation cycle / 360 quarter feedback review …)<br/>
🏥Company-paid Health Insurance andPersonal Services Vouchers (CESU)<br/>
🚆Commuter benefit coverage - Public Transport and Bike refund<br/>
⛱️Paid Time off – RTT and Saving time plan (CET)<br/>
✅Meal Vouchers<br/>
🎡Workers representatives committee(sports membership/cinemas vouchers/gift vouchers/discount)<br/>
<br/>
</p><p><br/>
<u>Feel free to explore Dailymotionculture a little further, please check out:<br/>
<br/>
</u></p><ul><li>Dailymotion.com</li><li>New-York office - BuiltIn</li><li>Offices in France - Welcome to the Jungle</li><li>Our articles</li></ul>
<!-- --> </span>
</div>"
12,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<strong><u>Company Description</u></strong><p><br/>
</p>We are music and tech fans hailing from all over the globe, working to make Deezer the most personal music streaming service.<p><br/>
</p>From data scientists to tech experts, artists &amp; labels specialists to marketers, and even in-house music editors, our team is spreading the love for music to over 180 countries. Supporting local and international artists and bringing them closer to their fans is our mission - we believe music is about diversity, multiculturalism and togetherness.<p><br/>
</p>Ready to join the team? We're all ears.<p><br/>
</p><strong><u>Job Description</u></strong><p><br/>
</p>As part of the Operational Performance team, you will work with data analysts and data scientists on a mission to improve performance and decision-making across commercial, product and content topics.<p><br/>
</p>You will be responsible for providing them with trusted data, innovative data solutions and collaborating with the central data team on high-impact, cross-functional projects aimed at scaling our data platform and operations.<p><br/>
</p><strong><u>What You Will Do</u></strong><p><br/>
</p><ul><li>Develop a deep understanding of the business and the analysts’ needs</li><li>Build and adopt software tools that help data scientists, analysts and business teams work more efficiently</li><li>Collaborate closely with the central Data Engineering team to design &amp; implement the best technical solutions</li><li>Design &amp; develop reliable data pipelines and be accountable for them</li><li>Design &amp; maintain easy to use data models, ensure their quality</li><li>Maintain data models documentations &amp; definitions available to all in our Data Catalog</li><li>Produce and maintain documentation and tutorials</li><li>Ensure data trainings for Data Analysts, developers and business users</li><li>Mentor analysts and data scientists on best practices (e.g., building testing suites and CI pipelines); driving enforcement of standards, tools and methodologies</li><li>Participate in projects planning, standups and retrospectives</li><li>Participate in the ideation process with our product managers &amp; tech leaders</li><li>Communicate proactively and manage technical issues</li></ul> Qualifications<p><br/>
</p><strong><u>What We Are Looking For</u></strong><p><br/>
</p><ul><li>2 years experience as a data engineer, BI engineer, analytics engineer or any similar role</li><li>Experience in designing, building and maintaining efficient &amp; reliable data pipelines</li><li>Proficient in SQL and with BI tools such as Tableau, Looker, …</li><li>DevOps : Docker, Ops, ETL/ELT</li><li>Proficient in Python, Scala Spark is a plus</li><li>Hands-on experience on cloud technologies is a plus (Ex. Google Cloud Platform)</li><li>Strong engineering skills (code design and quality, tests, reviews, logging, monitoring, continuous integration)</li><li>Passion for technology, industry research and solving business problems</li><li>Excellent presentation and communication skills: ability to interact with stakeholders with different levels of technical expertise</li><li>Understanding of data management, data quality and data governance</li><li>Ability to own and drive projects with a creative mindset</li><li>Team player attitude</li><li>Fluent in English</li><li>Hunger for learning</li><li>Affinity with the music industry </li></ul><p><br/>
</p>Additional Information<p><br/>
</p><strong>Life @ Deezer</strong><p><br/>
</p><ul><li>Flexible working policy with up to 10 days/month remote work</li><li>Home of music : deezer premium family subscription offered, opportunity to attend private artists sessions, amazing deezer parties for employees</li><li>English or French language courses open to all</li><li>Gymlib partnership + gym room and yoga/pilates classes on site</li><li>Headquarters in the heart of Paris, at home vibe and rooftop with amazing city views</li><li>Cafeteria stocked with free drinks and snacks daily and weekly casual drinks</li><li>Hackathons &amp; meetups</li></ul><p><br/>
</p><strong>If you feel like this is the right opportunity for you, press play!</strong><p><br/>
</p><i>We are an equal opportunity employer.</i>
<!-- --> </span>
</div>"
13,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
                # You Are :<br/>
<br/>
- You have at least 3 years of Data Engineering experience within a tech company<br/>
- You have built and operated data pipelines for real customers in production systems<br/>
- You know how to put machine learning models in production<br/>
- You have a master from an engineering school<br/>
- You are able to communicate clearly in English and French<br/>
<br/>
# Environment:<br/>
<br/>
- Python, Docker, GCP, Kubernetes, Spark, Elasticsearch
<!-- --> </span>
</div>"
14,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<strong>DATA ENGINEER GCP<br/>
<br/>
</strong>Paris - FRANCE<br/>
<br/>
Open Full Remote<br/>
<br/>
55K - 75K<br/>
<br/>
Cette belle start-up dans le monde du risque recherche un Data Engineer pour une création de poste afin d'épauler l'équipe en place.<br/>
<br/>
Son objectif ? Aider ses clients en leur facilitant l'accès à des données liées à leurs domaines d'activités afin d'anticiper des risques potentiels ;<br/>
<br/>
Pour le poste, on parle de Big data, Cloud et d'implémenter une culture DataOps.<br/>
<br/>
<strong><u>Vos Missions<br/>
</u></strong><ul><li>Être le lead technique sur les sujets liés à la data engineering</li><li>Implémenter une culture DataOps</li><li>Intégrer un gros volume de données</li><li>Construire et maintenir des pipelines de données</li><li>Optimisation de la plateforme</li><li>Participer à la croissance de la start-up<br/>
</li></ul><strong><u>Profil Recherché<br/>
</u></strong><ul><li>Bac +4/5 en Ecole de Commerce, Ecole d'Ingénieur, ou un diplôme équivalent</li><li>Vous justifiez d'au moins 2ans d'expérience en data engineering</li><li>Python et SQL sont votre langage de tous les jours</li><li>Bonne connaissance sur GCP et Spark</li><li>La Big data n'a pas de secret pour vous</li><li>Vous êtes sensible à travailler dans un environnement start-up</li><li>Vous aimez le travail d'équipe</li><li>Vous serez parfait si en plus de tout cela, vous savez maitriser Big Query<br/>
</li></ul>Si vous êtes intéressé, merci de postuler à ou directement via l'annonce.<br/>
<br/>
Mots clés : Risk / Start-up / Python / GCP/ Spark
<!-- --> </span>
</div>"
15,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<p>La donnée est votre passion et vous avez la conviction que son exploitation guide les besoins métier de demain. Pour vous, l’industrialisation des traitements sur la donnée est indispensable ?<i> </i>Venez définir et implémenter avec nous les architectures <i>Data Centric </i>de nos clients !</p><p><br/>
</p><p><strong>PROFIL</strong></p><p><br/>
</p><ul><li>F/H ;</li><li>Vous justifiez d’une expérience significative (au moins 4 ans) en <strong>développement</strong> et avez déjà touché à des <strong>technologies Big Data</strong> telles que Spark, Hadoop, Kafka ou autre.</li><li>Une connaissance d’un des écosystèmes cloud parmi AWS, Azure ou GCP est un plus.</li><li>Les bonnes pratiques de développement, telles que le TDD ou le pair programming, ne vous sont pas inconnues.</li><li>Vous pensez que pour bien conseiller, il faut maîtriser la réalisation.</li><li>Vous avez la conviction que<strong> la donnée a toute son importance</strong>, et pensez qu’elle est à mettre en regard des besoins métiers. Bref, vous aimez la donnée et êtes pragmatique.</li><li>Vous avez le sens de l’écoute et aimez <strong>partager</strong>. Les <strong>sujets technologiques vous passionnent</strong>, vous êtes à l’aise pour conseiller voire challenger les demandes de vos clients.</li><li>Vous aimez vous tenir au courant des nouvelles évolutions technologiques, et pratiquez une <strong>veille</strong> régulière.</li><li>Vous souhaitez vous confronter et contribuer à une<strong> communauté de spécialistes</strong>, qui ont à cœur l’évolution des pratiques et l’<strong>apprentissage continu</strong>.</li></ul><p><br/>
</p><p><strong>DESCRIPTIF</strong></p><p><br/>
</p><p><strong>Votre mission sera de permettre la réalisation de projets agiles qui comportent des enjeux majeurs autour du traitement de données, ont du sens, répondent à un besoin métier et s’inscrivent dans la durée.</strong></p><p><br/>
</p><p>Les Octos se différencient par <strong>leurs expertises et leurs manières de faire</strong> :</p><p><br/>
</p><ul><li>Nous n’hésitons pas à <strong>challenger un client</strong> quand l’expérience permet de proposer une solution plus adaptée.</li><li>Nous avons à cœur la <strong>transmission des compétences</strong> au client pour permettre à ce dernier de devenir autonome (le savoir n’est pas chasse gardée !).</li><li>Nous préconisons la mise en place d’un<strong> accompagnement méthodologique</strong>, au-delà du conseil.</li><li>Nous ne nous contentons pas d’émettre des préconisations autour de la donnée, des pipelines de traitement et de l’industrialisation : nous les <strong>développons</strong> et nous les <strong>mettons en œuvre</strong>.</li></ul><p>Le tout en s’appuyant sur une communauté de spécialistes <strong>de haute volée</strong>.</p><p>Nos missions de data engineering :</p><ul><li>Audit et cadrage de projets (architecture, méthodologie, qualité de code),</li><li>Réalisation/accompagnement de projets impliquant la mise en place d’architectures Data Centric,</li><li>Former et évangéliser sur les différentes technologies que nous utilisons.</li></ul><p><br/>
</p><p>Mais ce que nous cherchons avant tout, ce sont des personnalités qui enrichiront OCTO. Nous les reconnaissons à leur volonté de participer à l’<strong>amélioration de la vie de l’entreprise</strong>, de construire la vision et les <strong>offres de demain</strong>, de partager leurs connaissances pour faciliter la <strong>montée en compétences</strong> réciproque. De rejoindre, enfin, une communauté qui n’a pas peur d’affirmer sa différence.</p>
<!-- --> </span>
</div>"
16,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
                Vos missions au quotidien<br/>
<br/>
Vous savez « faire parler » les données pour créer de la valeur ? Vous souhaitez travailler en mode agile ? Rejoignez les Infrastructures Informatiques d’un Groupe en pleine transition numérique !<br/>
<br/>
Dans le cadre d’une équipe agile, en tant qu'alternant(e) Data Engineer, vous garantissez l’accès aux données liées à l’activité de la banque. Vous implémentez et industrialisez des traitements pour collecter, nettoyer et synthétiser des données afin de les rendre accessibles tout en veillant à leur qualité.<br/>
<br/>
Concrètement, vous serez amené(e), sous la supervision de votre tuteur et/ou votre manager, à :<br/>
<ul><li>Concevoir des solutions pour collecter, transformer et exploiter des gros volumes de données</li><li>Participer à l’industrialisation des traitements et à leur amélioration continue pour qu’ils soient fiables, robustes, performants et résilients afin de répondre aux exigences des partenaires métiers</li><li>Assurer la maintenance et l’évolution des différents pipelines de traitement</li><li>Assurer une veille technologique afin d’être à la pointe des connaissances en matière de data<br/>
<br/>
</li></ul>Et si c’était vous ? <ul><li>Vous préparez un Bac +4/5 en école d'Ingénieur ou Université avec une spécialisation en Informatique ou Ingénérie.</li><li>Vous souaitez intégrez une équipe dans un environnement tech : Hadoop Hortonworks, Kafka, Nifi, etc.</li><li>Vous êtes curieux(se) avec un bon esprit d’analyse et de synthèse.</li><li>Passionné(e) de data, vous proposez des améliorations et partagez avec votre équipe.</li><li>Vous souhaitez enrichir vos compétences techniques suivantes :</li><ul><li>Connaissances des outils Big data : Spark, Hive, Hadoop, NIFI, KAFKA, Elastic, Kibana.</li><li>Langage de développement : Scala, Python, Devops, outils CI/CD, microservice, API.</li><li>Connaissances des architectures Big Data : Résilience, architecture Kappa et lambda, couplage, event driven.</li></ul><li>You’re fluent in English! Vous êtes notre candidat(e) idéal(e) !<br/>
<br/>
</li></ul>Pensez à accompagner votre CV de votre planning de formation !<br/>
<br/>
Plus qu’un poste, un tremplin<br/>
<br/>
Dès votre arrivée, vous serez intégré(e) dans nos équipes et apprendrez chaque jour aux côtés de nos experts qui vous accompagneront dans vos missions. Progressivement, vous gagnerez en autonomie sur vos projets pour faire de cette expérience un vrai accélérateur de carrière. Vous découvrirez également toute la diversité de nos métiers, dans un secteur qui évolue et innove en permanence.<br/>
<br/>
En tant qu’alternant(e), vous pourrez bénéficier de jours de télétravail selon le rythme de votre service et celui de votre alternance, de notre mutuelle, de primes*de participation et intéressement, d’une prise en charge de 50% de votre titre de transport et de la billetterie à prix réduits de notre Comité d’Entreprise (concerts, cinéma, sport…). Lors de votre présence sur site, vous accéderez à une offre variée de restaurants d’entreprise et de caféterias à un tarif très compétitif.<br/>
<br/>
A la fin de vos études, diverses opportunités pourront s’offrir à vous, en France et à l’international.<br/>
<ul><li>Si vous avez 3 mois d’ancienneté sur l’exercice de référence.<br/>
<br/>
</li></ul>Pourquoi nous choisir ?<br/>
<br/>
Chez Société Générale, nous sommes convaincus que les personnes sont moteurs du changement, et que le monde de demain sera fait de toutes leurs initiatives, des plus petites aux plus ambitieuses.<br/>
<br/>
Que vous nous rejoigniez pour quelques mois, quelques années ou toute votre carrière, ensemble nous avons les moyens d’avoir un impact positif sur l’avenir.<br/>
<br/>
Créer, oser, innover, entreprendre font partis de notre ADN. Si vous aussi vous souhaitez être dans l’action, évoluer dans un environnement stimulant et bienveillant, vous sentir utile au quotidien et développer ou renforcer votre expertise, nous sommes faits pour nous rencontrer !<br/>
<br/>
Vous hésitez encore ?<br/>
<br/>
Sachez que nos collaborateurs peuvent s’engager quelques jours par an pour des actions de solidarité sur leur temps de travail : parrainer des personnes en difficulté dans leur orientation ou leur insertion professionnelle, participer à l’éducation financière de jeunes en apprentissage ou encore partager leurs compétences avec une association. Les formats d’engagement sont multiples.
<!-- --> </span>
</div>"
17,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
                Description de la mission<br/>
<br/>
La Direction des Systèmes d’Information du groupe Lacoste cherche à renforcer son équipe BI avec un Data Engineer dans l’équipe Back-Office BI. Le poste est ouvert dans une équipe de 5 personnes et reporte au BI Manager.<br/>
<br/>
Rôle:<br/>
<br/>
Participer au sein de l’équipe BI à la livraison des projets et à la gestion des demandes de support et d’évolution :<br/>
<br/>
Concevoir, développer et déployer les nouveaux flux de données :<br/>
<ul><li> Concevoir et enrichir les modèles de données à partir des spécifications fonctionnelles et/ou des expressions de besoin métier</li><li> Concevoir et développer des interfaces de chargement de données multi-sources</li><li> Recetter et déployer les développements réalisés</li><li> Rédiger et mettre à jour la documentation technique correspondante</li><li> Préparer et accompagner le passage en support<br/>
<br/>
</li></ul>Maintenir en condition opérationnelle les flux de données existants :<br/>
<ul><li> Suivre et améliorer les process d’alimentation mis en place</li><li> Enrichir les modèles de données métiers</li><li> Modifier, compléter les interfaces de chargement de données</li><li> Consommer des données de webservices / Api</li><li> Proposer des solutions d’optimisation des modèles existants<br/>
<br/>
</li></ul>Superviser le monitoring des flux de données :<br/>
<ul><li> Communiquer sur le statut des flux de données</li><li> Suivre les différents plans d’actions de résolutions d’incidents</li><li> Proposer des solutions de prévention des incidents<br/>
<br/>
</li></ul>Profil<br/>
<ul><li> Diplôme d’ingénieur ou équivalent (avec une première expérience stage/alternance etc...sur un poste similaire)</li><li> Doté d’une véritable culture du service : souci constant de l’utilisateur et de la qualité de la réponse à lui apporter</li><li> Bonne connaissance du cycle de développement de projets data</li><li> Autonome</li><li> Expérience de travail en équipe</li><li> Langue de travail : Français / Anglais<br/>
<br/>
</li></ul>Compétences Techniques :<br/>
<ul><li> ETL : Matillion, ODI 11</li><li> Base de données : Snowflake, Oracle 11g, SQL Server</li><li> SQL, PL/SQL, Transac SQL</li><li> Modélisation de données (Data Vault)</li><li> Autres compétences recherchées : Azure Data Factory<br/>
<br/>
</li></ul>Environnement Technique data Lacoste (en complément) :<br/>
<ul><li> Restitution : SAP Business Objects, QlikSense</li><li> Méthodologie Traditionnel et Agile</li><li> ETL : Matillion ; ODI; Azure Data Factory</li><li> Base de données : Oracle 11g, SQL server / Snowflake</li><li> Modélisation : Datavault / Star Shema</li><li> Data préparation : Python (« nice to have »)<br/>
<br/>
</li></ul>
<!-- --> </span>
</div>"
18,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<strong>About BlaBlaCar<br/>
<br/>
</strong>BlaBlaCar is the world’s leading carpooling platform, created with one dream in mind: leveraging technology to fill the millions of empty seats on the road. We offer long- and short-distance carpooling as well as a bus marketplace, with the mission to become the go-to marketplace for shared road mobility.<br/>
<br/>
Today, our community counts over 100 million travelers in 22 countries, creating a smarter, friendlier and carbon-saving transport network. Every year, our community saves 1.6 million tons of CO2e by sharing the road, equivalent to the CO2 emissions generated by Paris traffic in a year. But it doesn’t stop here – our team of 250+ engineers is developing innovative algorithms to further unlock the potential of shared travel and multiply its impact.<br/>
<br/>
We’re looking for people to join our journey – people who care, who are driven by impact and innovation, and who want to thrive in a fast-paced entrepreneurial environment. We offer a flexible workplace where we count on each other to take initiative. So join the ride – we can’t wait to see where it takes you.<br/>
<br/>
The team’s mission is to make BlaBlaCar the go-to marketplace to book a long-distance shared ride.<br/>
<br/>
For this the Data Pro Supply team develop and run Data products to improve the efficiency and productivity of the Bus supply; including the Operated Bus in Western Europe and the Pro MarketPlace business unit in Eastern Europe and Brazil. We build dashboards and algorithms to optimize the bus operations, evaluate the demand and provide key actionable insights for the business teams.<br/>
<br/>
Your role is to own BlaBlaCar’s passenger data from the raw layer to the warehouse layer. You will work closely with Data Analysts, Data Scientists and Software Engineers on your team to continuously provide data products to make decisions on the product.<br/>
<br/>
As well, you will contribute to the Data Engineering Chapter, a horizontal group of Data Engineers, whose mission is to improve tools for Data Engineers, and make efficient the day-to-day of a Data Engineer by innovative technologies, processes etc.<br/>
<br/>
Over the past 3 years, the scope covered by Data Engineers tripled, which led the Data Organization to reformulate as a Data Mesh. This setup allows producing data products faster and autonomously as a team dedicated to a specific data domain.<br/>
<br/>
Your Responsibilities<br/>
<ul><li>You will be building efficient and robust data models for large volumes of data</li><li>You will be developing and optimizing the data workflows from the raw layer to the data mart in our data platform</li><li>You will be implementing solutions to improve the data quality</li><li>You will take a central part in the Data Engineering Chapter, defining policies and standards for data management</li></ul>Your Qualifications<br/>
<ul><li>Experience using SQL (any SQL engine experience is appreciated; we use Bigquery)</li><li>Understanding of the ETL concept, as writing ETLs will represent a big part of your daily work</li><li>Experience working with a programming or scripting language (any is appreciated; we use Python with Cloud Composer/Airflow)</li><li>Basic knowledge of UNIX, GIT</li><li>Bonus: Experience with a Cloud provider (we use GCP)</li><li>Fluency in English - French would be a plus<br/>
<br/>
</li></ul><i>If you don’t meet 100% of the qualifications outlined above, tell us why you’d still be a great fit for this role in your application!<br/>
<br/>
</i>What we have to offer<br/>
<ul><li>🌎 An international environment: 45 nationalities across 6 countries: Brazil, France, Poland, Russia, Spain, and Ukraine</li><li>⚖️ A flexible workplace: with our hybrid remote setup and family-friendly policies, we are masters of our own schedules and work-life balance, no questions asked</li><li>💡 A culture of sharing: 360 onboarding weeks, weekly team-all BlaBlaTalks to learn about what other teams are up to, Q&amp;A sessions with our leadership, shared company KPIs, ‘Fail, Learn, Succeed’ moments where we destigmatize and share moments of failure with others</li><li>🚀 Innovation: Coding Nights to pitch ideas outside our roadmap and make dev’ dreams come true, weekly Product &amp; Tech Demos and blogs to share engineering stories, access to top conferences across Europe</li><li>🌱 Impact: building a product that has a real impact on society and the environment, and sharing an office culture that prioritizes low-waste and eco-friendly practices</li><li>💜 People-first: wind down from work at our weekly breakfasts and afterworks, or show off your talent at our annual BlaBlaShow</li><li>🧭 Shared company principles that guide us in our everyday decision-making and bring us closer to our goal. Find out more about our BlaBlaPrinciples</li><li>🚗Free carpooling &amp; 🚌 bus-rides wherever whenever</li></ul>Interested in joining the ride? Here’s what your hiring journey will look like.<br/>
<ul><li>a 30 min video-call with Elodie, our Talent Acquisition Manager to get to know you, understand your career expectations and answer your questions</li><li>a 45-min video-call interview with Celia, your future manager</li><li>A 45-min technical interview with: 45 min with some Data Engineers, Data Analysts and Software Engineers from the team</li><li>A 30-min informal discussion with Emmanuel, our VP Data</li></ul>BlaBlaCar is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.
<!-- --> </span>
</div>"
19,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<strong>Présentation de la direction générale et du service<br/>
<br/>
</strong>L'Institut d'émission des départements d'outre-mer (IEDOM) exerce ses missions au sein de l'Eurosystème, composé de la banque centrale européenne et des banques centrales nationales de la zone euro. L'IEDOM est chargé d'assurer la continuité territoriale des missions de banque centrale par délégation de la Banque de France dans les départements et collectivités d'outre-mer dont la monnaie est l'euro : Guadeloupe, Guyane, Martinique, Mayotte, La Réunion, Saint-Barthélemy, Saint-Martin, Saint-Pierre-et-Miquelon et les Terres australes et antarctiques françaises (TAAF).<br/>
<br/>
Vous êtes passionné(e) par la data, saisissez l'opportunité de nous accompagner dans le développement de notre socle de données et accélérons ensemble notre transformation digitale.<br/>
<br/>
Nous recrutons un(e) Data Engineer H/F qui sera notre leader technique pour l’acquisition, la transformation et la restitution de la donnée aux utilisateurs et aux applications de l’IEDOM . Rattaché(e) à la Division des Systèmes d’Information, vous travaillez au sein de l'équipe Etudes et Données - ETD. L'équipe ETD regroupe un ensemble d’experts et de chefs de projets en charge de la conception, de la réalisation et de l’assistance à l’exploitation des applications métiers (BI, Business analyste, chefs de projets informatiques pour les métiers). Votre mission est de rendre les données de l'entreprise accessibles et propres pour supporter et accélérer la création de valeur par les utilisateurs métiers et faciliter la transformation digitale.<br/>
<br/>
<strong>Descriptif de mission<br/>
</strong><ul><li> Vous concevez les flux de collecte, d'ingestion et de transformation des données pour alimenter notre socle de données internalisé, basé sur les technologies Talend – BO – JEDOX et participez à leur développement ainsi qu’à leur maintenance.</li><li> Vous mettez les données à la disposition des utilisateurs, dans le respect des normes et règles en vigueur (RGPD, sécurité, gouvernance interne...)</li><li> Vous faites de la veille technologique afin d'être force de proposition.</li><li> Vous êtes responsable : Des livraisons / De l'architecture data (flux, contrôles, sécurité, historisation, suppression etc.) / De la stratégie d'implémentation sur le socle internalisé / De la stratégie d'implémentation des flux de données avec Talend.</li><li> Vous assurez le support de niveau 2 aux incidents de production</li><li> Vous assurez le rôle de Lead développeur sur la plateforme DATA.<br/>
<br/>
</li></ul><strong>Profil recherché<br/>
<br/>
</strong>Vous êtes organisé(e), rigoureux(se) et appréciez le travail en équipe. Vous êtes naturellement curieux(se) et vous intéressez aux métiers et activités pour lesquels vous travaillez.<br/>
<br/>
De formation Bac +5 avec 5 ans d'expériences, vous maitrisez un ou plusieurs outils de type ETL/ELT (de préférence Talend).<br/>
<br/>
Vous avez une expérience significative et réussie sur l'implémentation d'un socle de données (Datawarehouse, Datalake, ...) dans un contexte On-premise.<br/>
<br/>
Vous avez une expérience sur les environnements BO.<br/>
<br/>
Vous êtes familier avec le concept DevOps et les méthodes agiles.<br/>
<br/>
Vous avez des connaissances en modélisation de données, MDM.<br/>
<br/>
Contactez nos ambassadeurs<br/>
<br/>
La Banque de France est une institution socialement responsable, attachée au respect de la diversité sous toutes ses formes, à la lutte contre les discriminations, à favoriser la parité Femme/Homme et à garantir un environnement de travail de qualité. Des aménagements de poste peuvent être organisés pour tenir compte des handicaps des personnes recrutées.
<!-- --> </span>
</div>"
20,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<strong>NOTRE AMBITION : Garantir la qualité de nos produits<br/>
<br/>
</strong>Permettre aux équipes, et à nos clients, d’accéder à des données fiables et de couvrir leurs besoins dans chaque domaine ! Nous développons :<br/>
<ul><li>Des reports à destination de nos clients.</li><li>Des calculs d’indicateurs stratégiques.</li><li>Des dashboards de pilotage, de launch control et de discovery.</li><li>Des reporting et des analyses de données.</li><li>Des contrôles industrialisés et des alertes.</li><li>Des flux de données avec nos partenaires.<br/>
<br/>
</li></ul><strong>NOTRE MÉTHODE DE TRAVAIL :<br/>
<br/>
</strong>Nous sommes 10 dans l’équipe Data (Head of Data, Team Lead BI, Data Analysts &amp; Data Engineers) répartis entre 2 squads. Rattachés au pôle Product &amp; Tech, nous collaborons avec toutes les équipes de PayPlug.<br/>
<br/>
Pour relever les challenges métiers et clients, nous construisons une plateforme de données sur GCP (Google Cloud Platform), sur laquelle nous développons nos pipelines et nos solutions. Celle-ci remplacera à terme intégralement notre système existant. Les développements se font principalement en SQL et Python.<br/>
<br/>
Notre stack technique :<br/>
<ul><li>Data : BigQuery, Cloud Composer/Apache Airlfow, Cloud Data Proc, Cloud PubSub</li><li>BI: Tableau</li><li>Infrastructure générale : Google Cloud Platform</li><li>Conteneur et Orchestration : Docker, Kubernetes</li><li>Versionning et CI/CD : gitlab &amp; gitlab-CI</li><li>Databases : BigQuery, MongoDB, MySQL, Vertica</li><li>Message broker : RabbitMQ<br/>
<br/>
</li></ul>Tes missions<br/>
<br/>
Tu intégreras la squad Data, composée d’un data engineer, d’un Lead Technique et rattachée au Head of Data. Le rôle de cette squad est clé dans la construction et le maintien de la plateforme de données !<br/>
<br/>
Tu assureras la mise en place des flux et modèles de données, en fonction des usages métiers.<br/>
<br/>
Tu seras garant de la qualité des données, du maintien des pipelines, de la documentation et de la performance de nos solutions.<br/>
<br/>
Dans tes missions tu seras aussi amené à échanger avec les autres équipes de Product &amp; Tech (SRE, squads Product &amp; Tech, squad BI, …).<br/>
<br/>
<strong>Requirements<br/>
</strong><ul><li>Data engineer avec au moins une première expérience. </li><li>Tu as déjà développé en SQL et python. </li><li>Tu as déjà travaillé sur une ou plusieurs plateformes cloud, de préférence GCP.</li><li>Tu as de l’expérience dans la construction, le maintien de pipelines de données, la modélisation, le monitoring et l’alerting.</li><li>Autonome et rigoureux.se, tu as de bonnes qualités relationnelles, ainsi qu’un bon esprit d’équipe.<br/>
<br/>
</li></ul><strong>Ce qui te démarque :<br/>
</strong><ul><li>De nature curieuse, tu aimes investiguer et comprendre les usages et pratiques en data engineering. </li><li>Tu as de l’expérience avec le production-grade code (design pattern, test unitaire et d’intégration, CI/CD, …)</li><li>Tu souhaites rejoindre une nouvelle équipe et construire une plateforme de données modernes et scalables. </li><li>À l’écoute, tu es soucieux.se de répondre aux besoins de tes collègues. <br/>
<br/>
</li></ul>Hiring Process<br/>
<ul><li>Appel de qualification avec Justine, Talent Managers (20-30') ;</li><li>Interview avec Mathilde, Head of Data</li><li>Test avec l’équipe Data (1h30) ;</li><li>Interview avec Christophe, VP Engineering (30’) </li><li>2 références.<br/>
<br/>
</li></ul><strong>Benefits<br/>
<br/>
</strong><strong>Modalités / avantages de travai</strong>l :<br/>
<ul><li>Organisation de travail hybride ;</li><li>25 CP / an &amp; 10 RTT / an ;</li><li>Des bureaux dans le 13e arrondissement de Paris (Bibliothèque François Mitterrand).<br/>
<br/>
</li></ul><strong>Avantages financiers</strong> :<br/>
<ul><li>Carte Apetiz (titres restaurants) d’une valeur de 9€ par jour (52% pris en charge) ;</li><li>Santé / Famille : Mutuelle santé Malakoff Humanis </li><li>Abonnements aux transports publics ou Velib pris en charge à 50% par PayPlug.<br/>
<br/>
</li></ul><strong>Autres</strong> :<br/>
<ul><li>Moka.care pour le soutien à la santé mentale de chacun ;</li><li>Windoo : activités de sport, bien-être et développement personnel en ligne ;</li></ul>
<!-- --> </span>
</div>"
21,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<strong><u>Description</u></strong><p><br/>
</p><i><strong>About Us:</strong></i><p><br/>
</p>Since 2015, Cognism has been a trusted leader in international sales intelligence, setting a new standard for data quality and compliance, trusted by 1000+ revenue teams worldwide.<p><br/>
</p>Cognism has helped thousands of sales teams to find, engage and close their dream prospects by providing them with a blend of real-time company, people and event data.<p><br/>
</p>One of our main goals, is to hire individuals who want to grow, develop and build on their already thriving careers and we work hard to help you achieve any goals you have, while having fun along the way.<p><br/>
</p>There hasn’t been a more exciting time to join us, we have just been through our Series C funding to enhance our global presence and we have just been named as one of the Top 15 startups in the UK by LinkedIn, for the 3rd year running!<p><br/>
</p>For more information on Cognism’s intuitive sales intelligence platform powered by world-leading data, compliance and targeting, please visit www.cognism.com.<p><br/>
</p><strong><u>Summary</u></strong><p><br/>
</p>We are now looking for an experienced Data Engineer to join our fast-growing team. You will be responsible for expanding and optimizing our data asset and data pipeline, as well as building new systems that collect, manage, and convert raw data into usable information for data scientists and business analysts to interpret.<p><br/>
</p>The ideal candidate is an experienced with big data pipelines and a natural data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.<p><br/>
</p>If you are detail-oriented, with excellent organizational skills and experience in this field, we’d like to hear from you.<p><br/>
</p><strong><u>Requirements</u></strong><p><br/>
</p><ul><li>Have a 5+ years of experience as a software engineer</li><li>Have a degree in Computer Science, Computer Engineering, Applied Statistics, Mathematics, or a related field (or you are in the process of achieving one).</li><li>Have some programming experience with Scala/Java/Python and SQL and are familiar with the principles of software engineering and data analytics.</li><li>Be excited to try new technologies and develop your technical skills.</li><li>Have know-how of designing and interacting with databases (SQL/NoSql).</li><li>Have experience with AWS-based database systems.</li><li>Be familiar with GIT and release engineering strategies.</li><li>Be proficient in spoken and written English.</li></ul><p><br/>
</p><strong><u>Responsibilities</u></strong><p><br/>
</p><ul><li>Optimizing and improving existing data pipelines</li><li>Building new systems that collect, manage, and convert raw data into usable information.</li><li>Support software developers, database architects, data analysts and data scientists on data initiatives</li><li>Participate in all phases of software development including concept, design, prototyping, and production release</li><li>Work directly with non-technical associates to understand business requirements</li></ul><p><br/>
</p><strong><u>Benefits</u></strong><p><br/>
</p><ul><li>Competitive salary based on the level of experience</li><li>24 days’ holiday PLUS all the usual public holidays</li><li>Flexible working hours, with the ability to work from home</li><li>Work with the latest technology paradigms</li><li>Access to education opportunities (books, courses, seminars, conferences)</li><li>Access to Cognism’s Employee Assistance Programme with Health Assured</li><li>Monthly Wellbeing Allowance</li><li>Remote Workouts and Social activities</li></ul>
<!-- --> </span>
</div>"
22,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
<p><strong>DATA ENGINEER – PLATEFORME SaaS - CDI</strong></p><p><strong>PARIS (75)</strong></p><p><strong>70K–80K EUR</strong></p><p><br/>
</p><p><strong> Stack : Python / SQL / BigQuery / Metabase / Jenkins / GCP</strong></p><p><br/>
</p><p><i>Dans le cadre d'une création de poste, rejoignez l'équipe Data Engineering d'une start-up spécialisée dans la création de contenu sur-mesure. </i></p><p><br/>
</p><p><strong>LE RÔLE</strong></p><p>Rattaché.e au VP Engineering et en binôme avec le Data Engineer actuel, vous interviendrez notamment sur les missions suivantes : </p><ul><li>Recueillir et challenger les besoins des équipes métiers,</li><li>Participer à l'enrichissement et l'amélioration de la plateforme Data en lien avec le Data Engineer actuel, </li><li>Développer des dashboardings et reportings à destination du business, </li><li>Veiller à la sécurité, scalabilité et flexibilité de la plateforme Data. </li></ul><p><br/>
</p><p><strong>VOTRE PROFIL</strong></p><ul><li>Vous justifiez de 4+ années d'expérience sur un poste similaire,</li><li>Vous êtes un.e expert.e SQL / Python,</li><li>Vous avez idéalement une première expérience dans un environnement GCP,</li><li>Vous disposez d'un excellent relationnel et avez une appétence pour l'aspect technico-fonctionnel.</li><li>Vous parlez couramment anglais.</li></ul><p><br/>
</p><p><strong>TELETRAVAIL</strong></p><ul><li>Full Remote possible, avec possibilité de télétravailler depuis l'étranger</li></ul><p><br/>
</p><p><strong>PROCESSUS DE RECRUTEMENT</strong></p><ul><li>Echange avec le Talent Acquisition Manager</li><li>Echange technique avec le VP Engineering et le Data Engineer actuel</li><li>Dernier échange avec le CTO</li></ul><p><br/>
</p><p><strong>COMMENT POSTULER ?</strong></p><p>Si vous êtes intéressé.e, merci de faire part de votre CV à Benjamin Remy.</p>
<!-- --> </span>
</div>"
23,"<div class=""jobs-box__html-content jobs-description-content__text t-14 t-normal jobs-description-content__text--stretch"" id=""job-details"" tabindex=""-1"">
<h2 class=""mt5 t-20 t-bold mb4"">
            À propos de l’offre d’emploi
          </h2>
<!-- -->
<!-- --> <span>
                Vos missions au quotidien<br/>
<br/>
Au sein de la Direction des Systèmes d’Information de la banque de détail France de la Société Générale vous …<br/>
<br/>
<strong>Bénéficierez d’un cadre de travail agréable </strong>facilitant l’équilibre vie pro/vie perso notamment en permettant jusqu’à 3 jours de télétravail par semaine.<br/>
<br/>
<strong>Evoluerez dans un environnement stimulant </strong>: créer, oser et innover, font partie de notre ADN. Alors si vous aimez être dans l’action, travailler sur les projets innovants et vous sentir utile au quotidien, n’hésitez plus ! Intégrez nos équipes afin de relever les défis qui nous animent au quotidien autour des <strong>pratiques de développement green, du cloud, de la data ou encore de la cybersécurité.<br/>
<br/>
</strong><strong>Baignerez dans une culture bienveillante : </strong>notre proximité avec le métier, l’entraide et l’écoute entre management et la communauté des experts font notre force. Un collectif plébiscité par 85% de nos collaborateurs*.<br/>
<br/>
<strong>En tant que data engineer, </strong>vous serez rattaché(e) à une Feature Team agile dédiée à la banque de financement et d'investissement au service des entreprises, des investisseurs et du public en France et dans le reste du monde.<br/>
<br/>
Détection de fraude en temps réel, anonymisation de données ou encore amélioration de moteurs de recommandation, vous créez des architectures qui répondent aux problématiques de masse de données, de rapidité et de volume. Bref, un(e) vrai(e) maestro de la data !<br/>
<br/>
Concrêtement, vous serez amené(e) à:<br/>
<ul><li>Concevoir des solutions pour collecter, nettoyer, organiser et synthétiser de gros volumes de données (pour alimenter bases de données, datalakes et projets Big Data) </li><li>Travailler en étroite collaboration avec les Data Architectes et Data Scientists pour industrialiser le procédé et produire des analyses opérationnelles </li><li>Participer à l’analyse complexe de données provenant de différentes sources </li><li>Assurer une veille technologique permettant de tester de nouvelles fonctionnalités et nouveaux outils <br/>
<br/>
</li></ul>Et si c’était vous ?<br/>
<br/>
Vous êtes curieux(se), avec un bon esprit d’analyse et de synthèse<br/>
<br/>
Passionné(e) de data, vous proposez des améliorations et partagez avec votre équipe<br/>
<br/>
You’re fluent in English!<br/>
<br/>
Incollable sur Spark et Scala, vous aimez également développer en Python<br/>
<br/>
Vous êtes diplômé(e) d’un Bac +5 en informatique / école d’ingénieur et avez une première expérience dans un environnement tech : Hadoop Hortonworks, Kafka, Nifi, etc.<br/>
<br/>
Plus qu’un poste, un tremplin<br/>
<br/>
<strong>A la Société Générale</strong>, nous sommes convaincus que les personnes sont moteurs du changement et que le monde de demain sera fait de toutes leurs initiatives, des plus petites aux plus ambitieuses.<br/>
<br/>
Aux 4 coins du monde, que vous nous rejoigniez pour quelques mois, quelques années ou toute votre carrière, ensemble nous avons les moyens d’avoir un impact positif sur l’avenir. Créer, oser, innover, entreprendre font partie de notre ADN.<br/>
<br/>
Si vous aussi vous souhaitez être dans l’action, évoluer dans un environnement stimulant et bienveillant, vous sentir utile au quotidien et développer ou renforcer votre expertise, nous sommes faits pour nous rencontrer !<br/>
<br/>
Vous hésitez encore ?<br/>
<br/>
Sachez que nos collaborateurs peuvent s’engager quelques jours par an pour des actions de solidarité sur leur temps de travail : parrainer des personnes en difficulté dans leur orientation ou leur insertion professionnelle, participer à l’éducation financière de jeunes en apprentissage ou encore partager leurs compétences avec une association. Les formats d’engagement sont multiples.<br/>
<br/>
Pourquoi nous choisir ?<br/>
<br/>
Rejoindre notre communauté Data, c’est se donner la chance d’avoir une vraie carrière d’expertise : monter en compétences grâce à nos parcours de formation ou encore participer à des événements tech et pouvoir échanger avec ses pairs issus des 4 coins du monde. Vous pourrez évoluer vers les métiers de Data Scientist, Architecte technique ou Expert Développement SI.
<!-- --> </span>
</div>"
24,"Good grasp of big data infrastructures and relevant big data tools.
Fusion of extremely structured, semi-structured and unstructured data sources (e.g.…"
25,"Uphold data security and data governance best practises.
You will have hands-on data processing and data modeling experience in a “big data” environment, with…"
26,The Data Access team is a new team at Indeed which is responsible for building systems that administer and enforce Indeed’s access policies over hundreds of…
27,"Design and plan enterprise-scale cloud environments including application dependencies, data storage and flow, network connectivity and overall cloud hosting."
28,We provide paid training in technical and soft skills to take your skills to the next level in exchange for a 12 month commitment to work with an established…
29,"The successful candidate will have hands-on data processing and data modeling experience in a “big data” environment.
Seattle 132,000 - 192,000 USD per year."
30,"Load and performance test data pipelines built using the above-mentioned technologies.
Knowledge or experience in architectural best practices in building data…"
31,"Proficient with data warehouse architecture and data pipelines (ELT/ETL, data modeling).
6 years’ experience with data & analytics toolsets, including data…"
32,"Interfacing with other engineers to extract, transform, and load (ETL) data from a wide variety of in-house and third-party data sources."
